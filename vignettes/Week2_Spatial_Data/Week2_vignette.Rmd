---
title: "Week 2: Spatial Data"
author: "Helene Wagner"
date: "`r Sys.Date()`"
show_toc: true
output:
  knitr:::html_vignette:
    toc: yes
    fig_width: 4 
    fig_height: 3.5
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Week 2: Spatial Data}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
## 1. Overview of Worked Example

Author: Helene Wagner

This code builds on data and code from the 'GeNetIt' package by Jeff Evans and Melanie Murphy.

### a) Goals 

This worked example shows:

- How to import spatial coordinates and site attributes as spatially referenced data.  
- How to plot raster data in R and overlay sampling locations.
- How to calculate landscape metrics.
- How to extract landscape data at sampling locations and within a buffer around them.

Try modifying the code to import your own data!

### b) Data set

This code uses landscape data and spatial coordinates from 30 locations where Colombia spotted frogs (*Rana luteiventris*) were sampled for the full data set analyzed by Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set.

- ralu.site: SpatialPointsDataFrame object with UTM coordinates (zone 11) in slot @coords and 17 site variables in slot @data for 31 sites. The data are included in the 'GeNetIt' package, for meta data type: ?ralu.site

We will extract values at sampling point locations and within a local neighborhood (buffer) from six raster maps (see Murphy et al. 2010 for definitions), which are included with the 'GeNetIt' package as a SpatialPixelsDataFrame called 'rasters':

- cti:   Compound Topographic Index ("wetness")
- err27: Elevation Relief Ratio 
- ffp:   Frost Free Period
- gsp:   Growing Season Precipitation
- hli:   Heat Load Index
- nlcd:  USGS Landcover (categorical map)

### c) Required R libraries

```{r message=FALSE, warning=TRUE}
# require(LandGenCourse)
require(sp)
require(raster)
require(GeNetIt)
```

Package 'tmaptools' not automatically installed with 'LandGenCourse':

```{r message=FALSE, warning=TRUE}
if(!require(tmaptools)) install.packages("tmaptools")
```

## 2. Import site data from .csv file

### a) Import data into 'SpatialPointsDataFrame'

The site data are already in a SpatialPointsDataFrame named 'ralu.site' that comes with the package 'GeNetIt'. Use 'data(ralu.site)' to load it. This will create an object 'ralu.site'. 

To demonstrate how to create a SpatialPointsDataFrame, we create a simple data frame 'Sites' with the coordinates and site data.

```{r}
data(ralu.site)
class(ralu.site)
Sites <- data.frame(ralu.site@coords, ralu.site@data)
class(Sites)
head(Sites)
```

To illustrate importing spatial data from Excel, here we export the data as a csv file, import it again as a data frame, then convert it to a SpatialPointsDataFrame. First we create a folder 'output' if it does not yet exist. 

Note: to run the code, remove all the hashtags '#" at the beginning of the lines to uncomment them. This part assumes that you have writing permission on your computer. Alternatively, try setting up your R project folder on an external drive where you have writing permission.

The second line exports the data in 'Sites' as a .csv file. The third line re-imports the .csv file to re-create data frame 'Sites'.

```{r}
#require(here)
#if(!dir.exists(paste0(here(),"/output"))) dir.create(paste0(here(),"/output"))
#write.csv(Sites, file=paste0(here(),"/output/ralu.site.csv"), 
#          quote=FALSE, row.names=FALSE)
#Sites <- read.csv(paste0(here(),"/output/ralu.site.csv"), header=TRUE)
```

The dataset 'Sites' contains two columns with spatial coordinates and 17 attribute variables. So far, R treats the spatial coordinates like any other quantitative variables. To let R know this is spatial information, we import it into a spatial object type, a 'SpatialPointsDataFrame' from the 'sp' package.

The conversion is done with the function 'coordinates', which takes a data frame and converts it to a spatial object of the same name. The code is not very intuitive. 

Note: the tilde symbol '~' (here before the first coordinate) is often used in R formulas, we will see it again later. It roughly translates to 'is modeled as a function of'.

```{r}
Sites.sp <- Sites
coordinates(Sites.sp) <- ~coords.x1+coords.x2
```

Now R knows these are spatial data and knows how to handle them. It does not treat the coordinates as variables anymore, hence the first column is now 'SiteName'.

### b) Add spatial reference data

Before we can combine the sampling locations with other spatial datasets, such as raster data, we need to tell R where on earth these locations are (georeferencing). This is done by specifying the 'Coordinate Reference System' (CRS) or a 'proj4' string. 

For more information on CRS, see: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf

We know that these coordinates are UTM zone 11 (Northern hemisphere) coordinates, hence we can use a helper function to find the correct 'proj4string', using function 'get_proj4' from the 'tmaptools' package. (For the Southern hemisphere, you would add 's' after the zone: "utm11s"). Here we call the function and the package simultaneously (this is good practice, as it helps keep track of where the functions in your code come from).

```{r}
# sp::proj4string(Sites.sp) <- tmaptools::get_proj4("utm11")
```

If we had longitude and latitude coordinates, we would modify the command like this: 
proj4string(Sites.sp) <- tmaptools::get_proj4("longlat"). 

### c) Access data in 'SpatialPointsDataFrame'

As an S4 object, Sites.sp has predefined slots. These can be accessed with the @ symbol:

- @data: the attribute data
- @coords: the spatial coordinates
- @coords.nrs: the column numbers of the input data from which the coordinates were taken (filled automatically)
- @bbox: bounding box, i.e., the minimum and maximum of x and y coordinates (filled automatically)
- @proj4string: the georeferencing information

```{r}
slotNames(Sites.sp)
```

Here are the first few lines of the coordinates:

```{r}
head(Sites.sp@coords)
```

And the proj4 string:
Let's compare this to the proj4string of the original 'ralu.site' dataset

```{r}
Sites.sp@proj4string
```
The default for 'get_proj4("utm11")' results in a slightly different proj4string than the 'ralu.site' dataset. The difference is in the 'datum' argument ('WGS84' vs. 'NAD83'): 

```{r}
ralu.site@proj4string
```

Let's go with the original information and copy it:

```{r}
Sites.sp@proj4string <- ralu.site@proj4string
```

## 3. Display raster data and overlay sampling locations, extract data 

### a) Display raster data

The raster data for this project are already available in the package 'GeNetIt', under the name 'rasters', and we can load them with 'data(rasters)'. They are stored as a 'SpatialPixelsDataFrame', another S4 object type from the 'sp' package.

```{r}
data(rasters)
class(rasters)
```

However, raster data are better analyzed with the package 'raster', which has an object type 'raster'. Let's convert the data to a 'RasterStack' of 'RasterLayer' objects (i.e. a set of raster layers with the same spatial reference information).

```{r}
RasterMaps <- stack(rasters)
class(RasterMaps)
```
Printing the name of the raster stack displays a summary. A few explanations:

- **dimensions**: number of rows (nrow), number of columns (ncol), number of cells (ncell), number of layers (nlayers). So we see there are 6 layers in the raster stack.
- **resolution**: cell size is 30 m both in x and y directions (typical for Landsat-derived remote sensing data)
- **coord.ref**: projected in UTM zone 11, though the 'datum' (NAD83) is different than what we used for the sampling locations. 

```{r}
RasterMaps
```

Now we can use 'plot', which knows what to do with a raster stack.

Note: layer 'nlcd' is a categorical map of land cover types. See this week's bonus materials for how to better display a categorical map in R.

```{r fig.width=8, fig.height=5.5}
plot(RasterMaps)
```
Some layers seem to show a similar pattern. It is easy to calculate the correlation between quantitative raster layers. Here, the last layer 'ncld', is in fact categorical (land cover type), and it's correlation here is meaningless.

```{r}
layerStats(RasterMaps, 'pearson', na.rm=T)
```

### b) Change color ramp, add sampling locations 

We can specify a color ramp by setting the 'col' argument. The default is 'terrain.colors(255)'. Here we change it to 'rainbow(9)', a rainbow colorpalette with 9 color levels.

Note: To learn about options for the 'plot' function for 'raster' objects, access the help file by typing '?plot' and select 'Plot a Raster* object'.

We can add the sampling locations (if we plot only a single raster layer). Here we use 'rev' to reverse the color ramp for plotting raster layer 'ffp', and add the sites as white circles with black outlines.

```{r fig.width=4.45, fig.height=4}
par(mar=c(3,3,1,2))
plot(raster(RasterMaps, layer="ffp"), col=rev(rainbow(9)))
points(Sites.sp, pch=21, col="black", bg="white")
```

### c) Extract raster values at sampling locations

The following code adds six variables to the data slot of Sites.sp. Technically we combine the columns of the existing data frame 'Sites.sp' with the new columns in a new data frame with the same name. 

R notices the difference in projection (CRS) between the sampling point data and the rasters and takes care of it, providing just a warning. 

```{r}
Sites.sp@data <- data.frame(Sites.sp@data, extract(RasterMaps, Sites.sp))
```
What land cover type is assigned to the most sampling units? Let's tabulate them.

Note: land cover types are coded by numbers. A total of 21 sites are classified as '42'. Check here what the numbers mean: https://www.mrlc.gov/nlcd06_leg.php

```{r}
table(Sites.sp@data$nlcd)
```

## 4. Calculate landscape metrics

We are going to use the package [`landscapemetrics`](https://r-spatialecology.github.io/landscapemetrics/). It is an R package to calculate landscape metrics in a tidy workflow (for more information about tidy data see [here](https://www.jstatsoft.org/article/view/v059i10/)). `landscapemetrics` is basically a reimplementation of ['FRAGSTATS'](https://www.umass.edu/landeco/research/fragstats/fragstats.html), which allows an integration into larger workflows within the R environment. The core of the package are functions to calculate landscape metrics, but also several auxiliary functions exit.

```{r, message=FALSE, warning=TRUE}
require(landscapemetrics)
require(dplyr)
```

To facilitate an integration into larger workflows, `landscapemetrics` is based on the [`raster` package](https://CRAN.R-project.org/package=raster). To check if a raster is suitable for `landscapemetrics`, run the `check_landscape()` function first. The function checks the coordinate reference system (and mainly if units are in meters) and if the raster values are discrete classes. If the check fails, the calculation of metrics is still possible, however, especially metrics that are based on area and distances must be used with caution.

```{r}
nlcd <- raster(RasterMaps, layer = "nlcd")

landscapemetrics::check_landscape(nlcd)
```

There are three different levels of landscape metrics. Firstly, metrics can be calculated for each single patch (a patch is defined as neighbouring cells of the same class). Secondly, metrics can be calculated for a certain class (i.e. all patches belonging to the same class) and lastly for the whole landscape. All these levels are implemented and easily accessible in `landscapemetrics`. 

All functions to calculate metrics start with `lsm_` (for landscapemetrics). The second part of the name specifies the level (patch - `p`, class - `c` or landscape - `l`). Lastly, the final part of the function name is the abbreviation of the corresponding metric (e.g. `enn` for the Euclidean nearest-neighbor distance). To list all available metrics, you can use the `list_lsm()` function. The function also allows to show metrics filtered by level, type or metric name. For more information about the metrics, please see either the corresponding helpfile(s) or [https://r-spatialecology.github.io/landscapemetrics](https://r-spatialecology.github.io/landscapemetrics/reference/index.html).

```{r}
landscapemetrics::list_lsm(level = "landscape", type = "diversity metric")

landscapemetrics::list_lsm(metric = "area")

landscapemetrics::list_lsm(level = c("class", "landscape"), type = "aggregation metric", 
                           simplify = TRUE)
``` 

### a) Calculate patch-, class- and landscape level landscape metrics 

To calculate a single metric, just use the corresponding function. The result of all landscape metric functions is always an identical structured `tibble` (i.e. an advanced `data.frame`). The first coloumn is the layer id (only interesting for e.g. a `RasterStack`). The second coloumn specifies the level ('patch', 'class' or 'landscape'). The third coloumn is the class id (`NA` on landscape level) and the fourth coloumn is the patch id (`NA` on class- and landscape level). Lastly, The fith coloumn is the abbreviation of the metric and finally the corresponding value in the last coloumn.

```{r}
# calculate percentage of landscape of class
percentage_class <- lsm_c_pland(landscape = nlcd)

percentage_class
```

Because the resulting `tibble` is type stable, you can easily rowbind different metrics (even of different levels). 

```{r}
# calculate three different landscape metrics and rowbind the result to one result tibble
metrics <- rbind(
  landscapemetrics::lsm_c_pladj(nlcd), 
  landscapemetrics::lsm_l_pr(nlcd),
  landscapemetrics::lsm_l_shdi(nlcd)
  )

metrics
```

To calculate a larger set of landscape metrics, you can just use the wrapper `calculate_lsm()`. The arguments are similar to `list_lsm()`, e.g. you can specify the level or the type of metrics to calculate. Alternatively, you can also provide a vector with the function names of metrics to calculate to the `what` argument. However, watch out, for large raster and many metrics, this can be rather slow (set `progress = TRUE` to get an progress report on the console). Also, we suggest to not just calculate all available metrics, but rather think about which ones might be actually suitable for your research question.

```{r}
# calculate all patch-level metrics using wrapper
nlcd_patch <- landscapemetrics::calculate_lsm(landscape = nlcd,
                                              level = "patch")

nlcd_patch

# show abbreviation of all calculated metrics
unique(nlcd_patch$metric)

# calculate all aggregation metrics on landscape level
nlcd_landscape_aggr <- landscapemetrics::calculate_lsm(landscape = nlcd, 
                                                       level = "landscape", 
                                                       type = "aggregation metric")

nlcd_landscape_aggr 

# calculate specific metrics
nlcd_subset <- landscapemetrics::calculate_lsm(landscape = nlcd, 
                                               what = c("lsm_c_pladj", 
                                                        "lsm_l_pr", 
                                                        "lsm_l_shdi"))

nlcd_subset
```

The resulting `tibble` is easy to integrate into a workflow. For example, to get the ordered patch id of the 5% largest patches, the following code could be used.

```{r}
# the pipe operator %>% passes the resulting object automatically to the next function as first argument
id_largest <- nlcd_patch %>% # previously calculated patch metrics
  dplyr::filter(metric == "area") %>% # only patch area
  dplyr::arrange(-value) %>% # order by decreasing size
  dplyr::filter(value > quantile(value, probs = 0.95)) %>% # get only patches larger than 95% quantile
  dplyr::pull(id) # get only patch id

id_largest
```

Because the metric names are only abbreviated, there is also a way to include the full name in the results. For the wrapper, just set `full_name = TRUE`. For the rowbinded `tibble`, you can use the provided `tibble` called `lsm_abbreviations_names` that comes with the package and use e.g. `dplyr::left_join()` to combine it with your results. 

```{r}
# add full metrics name to result
nlcd_subset_full_a <- landscapemetrics::calculate_lsm(nlcd, 
                                                      what = c("lsm_c_pladj", 
                                                               "lsm_l_pr", 
                                                               "lsm_l_shdi"), 
                                                      full_name = TRUE)

# add full metrics name to results calculated previously using left_join()
nlcd_subset_full_b <- dplyr::left_join(x = nlcd_subset,
                                       y = lsm_abbreviations_names,
                                       by = c("metric", "level"))

nlcd_subset_full_a
nlcd_subset_full_b
``` 

# b) Calculate patch-level landscape metrics for 'Evergreen Forest'

To only get the results for class 42 (forest), you can just `dplyr::filter()` the `tibble` (or use any other subset method you prefer). 

```{r}
# class == 42 for forest
forest_patch_metrics <- dplyr::filter(nlcd_patch, class == 42)
```

All functions make heavy use of connected components labeling to delineate patches (neighbouring cells of the same class). To get all patches of every class you can just use `get_patches()`. To get only a certain class, just specify the `class` argument and the neighbourhood rule can be chosen between 8-neighbour rule or 4-neighbour rule.

```{r}
# connected components labeling of landscape
cc_nlcd <- landscapemetrics::get_patches(nlcd, directions = 8)

# show name of each class
sapply(cc_nlcd, function(x) names(x)) 

# the fourth list entry is class forest
cc_forest_a <- cc_nlcd[4]
cc_forest_b <- landscapemetrics::get_patches(nlcd, class = 42) # watch out: result is list with one entry

cc_forest_a
cc_forest_b
```

To plot the patches you can use the `show_patches()` function.

```{r}
# patches of class 42 (forest) and 52 (shrubland)
show_patches(landscape = nlcd, class = c(42, 52), labels = FALSE)
```

It is also possible to visualize only the core area of each patch using `show_cores()`. The core area is defined as all cells that are further away from the edge of each patch than a specified edge depth (e.g. 5 cells). 

```{r}
# no core area for edge depth = 5 for class 52; try edge_depth = 1 for comparison
show_cores(landscape = nlcd, class = c(42, 52), edge_depth = 5, labels = FALSE)
```

Lastly, you can plot the map and fill each patch with the corresponding metric value, e.g. the patch size, with the help of `show_lsm()` .

```{r}
# there are two very large patches in class 42
show_lsm(landscape = nlcd, class = c(42, 52), what = "lsm_p_area", labels = FALSE)
``` 

Let's add forest patch size to the 'Sites.sp' data. To extract landscape metrics of a patch in which sample points are located in, use `extract_lsm()`. Which metrics are extracted can be specified by the `what` argument (similar to `calculate_lsm()`). However, only patch level metrics are available. Please be aware, that the resulting `tibble` now has a new column, namely the id of the sample point (in the same order as the input points).

```{r}
# extract patch area of all classes
patch_size_sp <- extract_lsm(landscape = nlcd, y = Sites.sp, what = "lsm_p_area")

# because we are only interested in the forest patch size, we set all area of class != 42 to 0
patch_size_sp_forest <- dplyr::mutate(patch_size_sp, 
                                      value = dplyr::case_when(class == 42 ~ value, 
                                                               class != 42 ~ 0))
# add data to sp object
Sites.sp@data$ForestPatchSize <- patch_size_sp_forest$value
Sites.sp@data$ForestPatchSize
```

Plot a bubble map of forest patch size at each sampling location.

```{r fig.width=4.45, fig.height=4}
par(mar = c(3,3,1,2))
bubble(Sites.sp, "ForestPatchSize", fill = FALSE, key.entries = as.numeric(names(table(Sites.sp@data$ForestPatchSize))))
```

## 5. Sample landscape metrics in sampling plots

### a) Calculate metrics in buffer around sampling locations

`landscapemetrics` has a build-in function to sample metrics in a buffer around sample locations. You can choose the shape of the buffer (either a circle or a square) and of course which metrics to sample (similar to `calculate_lsm()`).

```{r}
# sample some metrics within buffer around sample location 
nlcd_sampled <- landscapemetrics::sample_lsm(landscape = nlcd, 
                                             what = c("lsm_l_ta", 
                                                      "lsm_c_np",
                                                      "lsm_c_pland", 
                                                      "lsm_c_ai"),
                                             shape = "square",
                                             y = Sites.sp, 
                                             size = 500)

nlcd_sampled
```

The `tibble` now contains two additional columns. Firstly, the `plot_id` (in the same order as the input points) and secondly, the `percantge_inside` of the sample plot within the landscape. In cases in which the sample location is on the edge of the landscape, the sample plot could only be partly within the landscape. The value can also deviate from 100% because the sample locations are not necessarily in the cell center and the actually clipped cells lead to a slightly smaller or larger sample area. A circle sample shape increases this effect.

It is also possible to get the clippings of the sample plots as a `RasterLayer`. Therefore, just set `return_raster = TRUE`.

```{r}
# sample some metrics within buffer around sample location and returning sample
# plots as raster
nlcd_sampled_plots <- landscapemetrics::sample_lsm(landscape = nlcd, 
                                                   what = c("lsm_l_ta",
                                                            "lsm_c_np",
                                                            "lsm_c_pland",
                                                            "lsm_c_ai"),
                                                   shape = "square",
                                                   y = Sites.sp, 
                                                   size = 500, 
                                                   return_raster = TRUE)

nlcd_sampled_plots
```

The result will be a nested `tibble` containing the `plot_id`, the metrics and a `RasterLayer` with the clipped sample plot (as a `list`).

```{r, fig.width=8, fig.height=5.5}
# show results for first four plots
nlcd_sampled_plots$raster_sample_plots[1:4]

par(mfrow = c(2,2))
  plot(nlcd_sampled_plots$raster_sample_plots[[1]], 
       main = paste(Sites.sp$SiteName[1]), 
       col = rev(rainbow(9)))
  plot(nlcd_sampled_plots$raster_sample_plots[[2]],
       main = paste(Sites.sp$SiteName[2]),
       col = rev(rainbow(9)))
  plot(nlcd_sampled_plots$raster_sample_plots[[3]],
        main = paste(Sites.sp$SiteName[3]), 
       col = rev(rainbow(9)))
  plot(nlcd_sampled_plots$raster_sample_plots[[4]],
        main = paste(Sites.sp$SiteName[4]), 
       col = rev(rainbow(9)))
par(mfrow = c(1,1))
```

### b) Extract landscape metric of choice for a single cover type (as vector)

To extract a metrics you can just `dplyr::filter()` the resulting `tibble` and pull the `value` coloumn.
 
```{r}
# filter the results for class == 42 (forest) and pland (percentage) and pull
# results as vector
percentage_forest_500_a <- dplyr::pull(dplyr::filter(nlcd_sampled, 
                                                     class == 42, 
                                                     metric == "pland"), value)

# # same workflow, but using a pipe
# percentage_forest_500_b <- nlcd_sampled %>% 
#   dplyr::filter(class == 42, 
#                 metric == "pland") %>% 
#   dplyr::pull(value)

percentage_forest_500_a
# percentage_forest_500_b
```

### c) Extract landscape metric of choice for all cover types (as data frame). 

To extract the landscape metric 'prop.landscape' for all cover types as a `tibble`, just `dplyr::filter()` the `tibble` again, but only use the `metric` as filter. 

```{r}
# filter for percentage of landscape
percentage_forest_500_df <- dplyr::filter(nlcd_sampled,
                                          metric == "pland")

percentage_forest_500_df
```

The percent cover of all cover types should add up to ~ 100% (i.e., 1) for each site. We can check this with the function `dplyr::summarize()`. First, we need to group the data using the `plot_id` and than sum all percentages.

```{r}
# group by plot_id and sum all percentages
pland_sum_a <- dplyr::summarize(dplyr::group_by(percentage_forest_500_df, 
                                                by = plot_id), 
                                sum_pland = sum(value))

# # same workflow, but using a pipe
# pland_sum_b <- percentage_forest_500_df %>% 
#   dplyr::group_by(plot_id) %>% 
#   dplyr::summarize(sum_pland = sum(value))

pland_sum_a
# pland_sum_b
```

### d) Extract all landscape metrics for a single cover type (as data frame)

Just `dplyr::filter()` for `class == 42` and add the sites names as coloumn to the resulting `tibble`.

```{r}
# filter for class == 42 (forest)
forest_500_df <- dplyr::filter(nlcd_sampled,
                               class == 42)

# data.frame with id and name of site
SiteName_df <- data.frame(id = 1:length(Sites.sp$SiteName), site_name = Sites.sp$SiteName)

# add site_name to metrics using plot_id and id of sampling sites
forest_500_df <- dplyr::left_join(forest_500_df, SiteName_df, by = c("plot_id" = "id"))

forest_500_df
```

Done!

Note: check this week's bonus material if you want to see how to use the new 'sf' library for spatial data, and how to export the site data to an shapefile that you can import into a GIS.

The following code detaches all packages except for some basic ones:

```{r message=FALSE, warning=TRUE, include=FALSE}
# LandGenCourse::detachAllPackages()
```
