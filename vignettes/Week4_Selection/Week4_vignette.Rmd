---
title: "Week 6: Selection"
author: 
- "Andrew Eckert"
- "Kati Csillery"
- "Helene Wagner"
date: "`r Sys.Date()`"
show_toc: true
output:
  knitr:::html_vignette:
    toc: yes
    fig_width: 4 
    fig_height: 3.5
vignette: >
  %\VignetteIndexEntry{Week 6: Selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## 1. Overview of Worked Example

### a) Goals 

**Justification**: Natural selection acts on phenotypic variation that is genetically determined. As such, it can be difficult to get a complete picture about adaptation from scanning genomes using molecular markers. The reason is that genetic outliers, even if true positives, have little to no information present about what phenotype they affect and how this phenotype results in fitness differences. Moreover, it is debatable to scan genomes for the presence of outliers if you have yet to demonstrate that the populations being sampled are locally adapted.

**Learning Objectives**: This lab was constructed to give you experience in working with basic quantitative and population genetic analyses useful to testing hypotheses about local adaptation. Phenotypic measurement is undergoing a revolution, so that familiarity with basic methods in quantitative genetics will serve you well in the future. By the end of the laboratory, you should be able to do the following:

- Construct, fit, and assess linear mixed models (LMMs) to estimate genetic values for a phenotypic trait measured for families existing in a common garden.
- Use LMMs to estimate heritability of a trait.
- Test trait differentiation among populations, and compare it to differentiation at neutral loci.
- Assess trait correlation with environmental variables.

### b) Data set 

The data with which you are working come from a study of western white pine (Pinus monticola Dougl. ex D. Don) sampled around the Lake Tahoe Basin of California and Nevada. These data consist of 157 trees sampled from 10 populations (n = 9 to 22 trees/population). Within each population, trees were sampled within three plots. For each plot, GPS coordinates were collected (i.e. each plot in each population has its own GPS coordinates) and used to generate a set of 7 environmental variables. From these trees, needle tissue was collected from which total DNA was extracted and genotyped for 164 single nucleotide polymorphisms (SNPs). Seeds were also collected and planted in a common garden. The seedlings (n = 5 to 35/tree) were measured for several phenotypic traits. The phenotypic trait we will be working with today is known as the carbon isotope ratio ($δ^{13}C$). It is the ratio of two isotopes of carbon ($^{13}C$ and $^{12}C$) relative to an experimental control, and it is strongly correlated with intrinsic water-use efficiency in plants. Plants need water to live, so it is not a stretch of the imagination to believe that this phenotypic trait has a link with plant fitness.

We will thus have the following data:

- **WWP_phenotype_data.txt**: Phenotypic measurements for 5 seedlings per tree made in a common garden.
- **WWP.ecogen**: an 'ecogen' object (package 'EcoGenetics') with SNP genotypes for all trees sampled in the field, and with environmental data collected from each plot within each population.

### c) Required R libraries

All required packages should have been installed already when you installed 'LandGenCourse'.

```{r message=FALSE, warning=TRUE}
require(LandGenCourse)
require(lme4)
#require(car)
#require(EcoGenetics)
require(tibble)
#require(vegan)
require(gstudio)
require(sjPlot)
require(MuMIn)

source(system.file("extdata", "supplemental_R_functions.R", 
                            package = "LandGenCourse"))
```

## 2. Estimate genetic values for an observed phenotypic trait

**Motivation**: A lot of genetics can be carried out without use of any molecular markers. Practitioners of empirical population genetics forget this quite often. A common garden allows us to create a standardized environment in which we minimize the influence of environment on the expression of a particular phenotypic trait. Since we know, after over a century of showing it to be true, that phenotypic variation results from genetic variation, environmental variation, and the interaction of genetic and environmental variation, then if we standardize the environment, phenotypic variation we see in the common garden is due to genetic variation (or if multiple gardens are used, genetics and the interaction of genetics and the environment).

**Goals & Background**: The goal for this part of the laboratory is to construct, fit, and assess LMMs for $δ^{13}C$. We will be using the data in the file named "WWP_phenotypic_data.txt". These data are organized in a tab-delimited text file with seedlings grouped by maternal tree (i.e. its mother tree), plot, and population. Also included is an experimental treatment known as “block”. In a common garden, seedlings from the same maternal tree are randomized among blocks to avoid the influence of micro-environmental variation on expression of phenotypic traits.

### a) Import phenytypic data 

```{r}
phen <- read.delim(system.file("extdata", "WWP_phenotype_data.txt", 
                            package = "LandGenCourse"), sep = "\t", header = T)
tibble::as.tibble(phen)
```

Check the variable types: 'family' and 'block' have been imported as type 'integer', and we need to convert them to 'factor' first.

```{r}
phen$family <- as.factor(phen$family)
phen$block <- as.factor(phen$block)
sapply(phen, class)
```

### b) Fit linear models to phenotypic trait data

Now,we are ready to fit a series of linear models. We will fit three models in total for this laboratory: 
- **mod1**: a fixed effect model with only an intercept, 
- **mod2**:a LMM with an intercept (fixed) and a random effect due to family, and 
- **mod3**:a LMM with an intercept, a random effect due to family nested within population, and a random effect of population. We will thus be ignoring the plot identifiers. All models will also have a fixed effect of block.


```{r}
mod1 <- lm(d13c ~ block, data = phen)
mod2 <- lme4::lmer(d13c ~ (1|family) + block,data = phen, REML = TRUE)
mod3 <- lme4::lmer(d13c ~ (1|population/family) + block, data = phen, REML = TRUE)
```

We are now ready to explore each of these models and to look at which model best fits our data. First, you can use the Anova() function in the car library to test the statistical significance of the fixed terms in each mod. This function let's us control the type of sums of squares (SS) being used, here type III sums of squares (see this week's video, Part 2).

```{r}
car::Anova(mod1, type="III")
car::Anova(mod2, type="III")
car::Anova(mod3, type="III")
```

Compare the output. What do you conclude about the effect of block in each of the models? Remember this is an experimental treatment, so your conclusion directly addresses whether micro-environmental variation exists in your common garden.

Note that 'ANOVA' returned the results fo an F-test for the model 'mod1' of class 'lm' and a chi-square test for the models of class 'lme4' that were fitted with 'lmer'.

### c) Compare model fit

Now, let’s explore which model best fits our data. To do this we will use the Akaike Information Criterion (AIC). This statistic is like a penalized likelihood score, where the penalty increases as the number of parameters in a model increases. When using AIC, the model with the lowest score is the preferred model. 

How do we ensure that the AIC values of the different models are comparable? 

- For 'mod1' that was fitted with 'lm', we use the function 'AIC'. 
- For the other two models that were fitted with 'lmer', we use 'extractAIC'. 

The function 'extractAIC' refits the models with 'REML=FALSE' to obtain AIC values that are comparable between models with different fixed effects. It returns two values, the first is the 'equivalent degrees of freedom' of the fitted model, and the second is the 'AIC' value. Here we only extract the second value.

```{R}
aic_vals <- c(AIC(mod1), extractAIC(mod2)[2], extractAIC(mod3)[2])
names(aic_vals) <- c("mod1","mod2","mod3")
aic_vals
```

Hence, model 'mod3' appears to be the best model. This suggests that there are important differences among populations, and among trees within populations.

We will learn more about model selection later in the course (Week 12).

### d) Check model validity and describe effects

Is the model 'mod3' valid? Let's check the residuals for deviation from normal distribution. The function 'plot_model' has an argument 'type="diag"' that returns three diagnostic plots for an 'lme4' object. 

```{r}
sjPlot::plot_model(mod3, type="diag") 
```

Hint: You may need to use the arrow symbols to scroll through the plots in the 'Plot' tab in RStudio.

The plots are:

- A normal probability plot: points should follow line.
- A density curve: distribution should approximate normal curve.
- A scatterplot of residuals against fitted values: the plot should not 'thicken' (which would indicate non-constanct variance = heteroscedasticity), and the blue line should be horizontal (no non-linear relationship).

The residual plots did not indicated any major problem. Hence we can procede to interpreting the results.


How much of the variation in 'd13c' is explained by the fixed and random factors?

The function 'r.squaredGLMM' from the package 'MuMIn' (which stands for multi-model inference) returns two values:

- R2m: marginal $R^2$. This is the variance explained by fixed factors. 
- R2c: conditional $R^2$. This is interpreted as the variance explained by both fixed and random factors, i.e., the entire model.

```{r}
MuMIn::r.squaredGLMM(mod3)
```

The fixed effect 'block' had a small effect of about 2.5%. The total model explained about 23%, most of which was due to the random effects.

How important are the nested random factors 'population' and 'family'? The summary for 'mod3' lists the variance components under 'Random effects'. 


```{r}
summary(mod3)
```

Here we extract these variance components from the summary and divide by their sum. 


```{r}
var.re <- unlist(summary(mod3)$varcor)
var.re / (sum(var.re) + summary(mod3)$sigma^2)
```

An easier way to accomplish the same thing is to use the function 'icc' of the 'sjstats' package. The inter-class correlation ICC tells us how much of the variance is explained by clustering within populations and families:

```{r}
sjstats::icc(mod3)
```

Thus population and family (nested within population) both had a medium-size effect (> 9% variance explained) on d13c values.

Which blocks, populations or trees had higher mean d13c values than others? To answer this question, we extract and plot the fixed and random effects. We'll start with the fixed effect for 'block'. 

The effects are not simply the mean of all seeds from the same block, but the marginal effect of 'block', i.e., estimates of the mean per block after accounting for all other effects (i.e., keeping population and family constant). 

```{r}
ggeffects::ggeffect(mod3, terms=c("block"))
sjPlot::plot_model(mod3, type="eff", terms=c("block"))
```

The plot shows the mean d13c value for each block (accounting for population and family) and a 95% for each mean.

Now let's plot the effects (mean with 95% confidence interval) for each random factor, accounting for all other factors. 

Note: If the plots appear in the 'Plot' tab in RStudio, use the arrows to scroll between plots.

```{r}
sjPlot::plot_model(mod3, type="re")     
```

We can see that there are considerable differences between populations, where 'blk cyn' has the highest, and 'mt watson' the lowest mean. 

### e) Calculate genetic values for maternal trees

Now that we have the best model and the effect estimates, let’s calculate the genetic values for each maternal tree for $δ^{13}C$. What we are after is the value of $δ^{13}C$ for each tree from which we measured $δ^{13}C$ from five of her offspring in the common garden. This is the genetic value and represents the value of $δ^{13}C$ that would result if you knew all the genes and effect sizes of variation within those genes determining variation for this trait (see genetics without molecular markers!).

Because the effect of each factor is calculated while accounting for all other factors, we need to piece together the genetic trait of each tree from three additive effects:

- Global mean (intercept)
- Family effect
- Population effect

We can extract the global mean from the object 'mod3'. This is the intercept listed in the model summary, and it is also the effect estimate for the first block. 

```{r}
Global.mean <- mod3@beta[1]
Global.mean
```

To get the effects due to family and population, we can use the 'ranef' function from package 'lme4'. This function produces estimates for each family and population in a list with named elements ('family' and 'population').

The family effect is straight-forward to extract. Each value is relative to the global mean, so that family '59' has a mean that is 0.2697 higher than the global mean. Each row name is a combination of the family (tree) ID and the population ID, separated by a colon ":".

```{r}
Family.effect <- lme4::ranef(mod3)$family
head(Family.effect)
```
Now we need, for each tree, the corresponding population effect. However, 'ranef' returns only one value per population:

```{r}
lme4::ranef(mod3)$population
```

Now we need to assign the correct value to each tree. Here we work from the row names of the family effect. The function 'strsplit' splits a character string at the symbol defined by 'split'. We use it to split the row names by the colon, then retain only the second half, whic is the population ID. We use function 'lapply' to do this for each row name of 'Family.effect'. This returns a list with only one character string in each list element. We use 'unlist' to reduce the list to a character vector 'pop.name'. 


```{r}
pop.name <- unlist(lapply(row.names(Family.effect), 
                        function(x) strsplit(x, split=":")[[1]][2]))
head(pop.name)
```

Next, we find for each pop name the corresponding row in the table with the population effects. The function 'match' returns the corresponding line number, which we write into a vector 'index'. 

```{r}
index <- match(pop.name, row.names(ranef(mod3)$population))
head(index)
```

Finally, we use the index to extract for each of the 157 trees the corresponding family effect value. 

```{r}
Population.effect <- ranef(mod3)$population[index,1] 
head(Population.effect)
```

Now we can calculate the trait from the global mean, family effect and population effect:

```{r}
Trait <- Global.mean + Family.effect + Population.effect
head(Trait)
```

The object Trait thus contains the phenotypic trait value for each maternal tree for $δ^{13}C$. Note that we did not measure the maternal tree, but inferred her phenotype from her offspring in a common environment.


## 3. Estimate trait heritability

**Motivation**: Now that we have learned how to estimate genetic values for $δ^{13}C$, let’s learn how to estimate what fraction of the total variation in trait values is due to genetic effects and how much of this genetic effect is due to families nested within populations and to populations. These analyses provide key information about whether or not local adaptation should even be considered. Remember that local adaptation is about genetically determined phenotypes that vary across environments in responses to differing selective pressures. This step allows us to assess how genetic variation for a phenotypic trait is distributed across the landscape.

**Goals & Background**: The goal for this part of the laboratory is to estimate heritability, trait differentiation, and correlation with environment for trait values determined in Part 1. To do this, we will be using the output from the previous part of the laboratory and the environmental data contained in the file named "WWP_environmental_data.txt". As with the phenotype file this is a tab-delimited text file.

### a) Estimate heritability

Let’s start with estimating the heritability of $δ^{13}C$. If you remember from your undergraduate evolution course, heritability refers generally to the proportion of phenotypic variance due to genetic variance. It comes in at least two different versions. The version we are interested in is narrow-sense heritability ($h^2$), which is defined as the ratio of additive genetic variance to total phenotypic variance:

$$h^{2} = \frac{\sigma^{2}_{additive}}{\sigma^{2}_{total}}$$
We need to extract the variance components from 'mod3' for all model terms. We do this visually by printing mod3 to screen or using a set of functions applied to 'mod3'. Here, we will do both.

```{r}
mod3
```

Using the results from above, let’s calculate $h^2$. If we assume that the seedlings from each maternal tree are half-siblings (i.e. same mom, but each with a different father) then $σ^2_A = 4 σ^2_{family}$ (so variance due to family:population). If the seedlings were all full-siblings, then the 4 would be replaced with 2. We also need to realize that we are using a hierarchical model, where some of the genetic effects are due to among populations, where $h^2$ is a measure within populations. That means we have to ignore the variance due to populations. Let’s assume half-siblings. We can then do the following:

Copying the values visually from the output above (note that we need to square the standard deviations to get the variances):

```{r}
add_var <- 4*(0.2857^2)
total_wp_var <- (0.2857^2) + (0.8452^2)
h2 <- add_var/total_wp_var
h2

```

And the same using functions to extract the variance components:

```{r}
add_var <- 4*var.re['family:population']
total_wp_var <- var.re['family:population'] + summary(mod3)$sigma^2
h2 <- add_var/total_wp_var
h2
```

Inspect your value of $h^2$. What does it mean? 

We have generated a point estimate for $h^2$. It represents the average $h^2$ across populations after removing the genetic effects due to population differences. Would it not be nice to also have a confidence interval? We can do that through an approach known as parametric bootstrapping. This approach simulates data using the fitted model a large number of times. Using the resulting distribution, you can create confidence intervals using the appropriate symmetric quantiles of the distribution. To see this, please
do the following using the mod_boot function in "supplemental_R_functions.R". It will takes a few moments to run the first line.

```{r}
par(mar=c(1, 2, 1, 1))
h2_boot_out <- mod_boot(model = mod3, nboot = 1000)
ci_95 <- quantile(h2_boot_out, probs = c(0.025, 0.50, 0.975))
ci_95
boxplot(h2_boot_out, range=5); abline(h = h2, col = "red") 
```

Interpret the numerical 95% confidence intervals 'ci_95' and boxplot with our original $h^2$ estimate (red line: 0.405) for comparison to the bootstrap distribution. 

- Do you think that $h^2$ is statistically different than zero? 
- Is this consistent with the AIC results from Part 1? 
- Is it meaningful that the red line is very similar to the mean (or median) of the bootstrap distribution? 
- How would you change the code for a 99% confidence interval?

### b) Estimate trait differentiation

Great, we have shown that within population genetic variation is statistically greater than zero. What about among population genetic variation? Let’s get to that right now. To measure among population genetic variation we will use a statistic known as $Q_{ST}$. It is similar in concept to $F_{ST}$ from population genetics. To estimate $Q_{ST}$, we will use our LMM output again. If we assume that all seedlings are again half-siblings, then:

$$Q_{ST} = \frac{\sigma^{2}_{population}}
{\sigma^{2}_{population}+8\sigma^{2}_{family}}$$

Again, we can do this by copying the values visually from the output:

```{r}
num_qst <- 0.3295^2
dem_qst <- 0.3295^2 + (8*(0.2857^2))
qst <- num_qst/dem_qst
qst
```

Or using functions to extract the variance components:

```{r}
num_qst <- var.re['population']
dem_qst <- var.re['population'] + (8*var.re['family:population'])
qst <- num_qst/dem_qst
qst
```

Inspect your value in qst object:

- What does it mean? 
- Look at the quantities in the equation above, what is the denominator equal to? Is it the total phenotypic variance or the total genetic variance?

Now, we can again look at a confidence interval using parametric bootstrapping. Again, please use the function 'mod_boot_qst' that is also located in "supplemental_R_functions.R". As before, it will take a few moments for the first line to finish.

```{r}
par(mar=c(1, 2, 1, 1))
qst_boot_out <- mod_boot_qst(model = mod3, nboot = 1000)
ci_95_qst <- quantile(qst_boot_out, probs = c(0.025, 0.50, 0.975)) 
ci_95_qst
boxplot(qst_boot_out); abline(h = qst, col = "red")
```

Interpret the results. 

- Do you think that $Q_{ST}$ is statistically different than zero? 
- Is this consistent with the AIC results from Part 1? 
- Is it meaningful that the red line is less similar to the mean (or median) of the bootstrap distribution as compared to $h^2$?

## 4. Compare $Q_{st}$ to $F_{st}$ measured from SNP data

### a) Import 'ecogen' object 

Load the data set 'WWP.ecogen'.

```{r}
#data(WWP.ecogen, package="LandGenCourse")
load(paste0(here::here(), "/data/WWP.ecogen.RData"))
```

The 'ecogen' object contains information in the following slots:

- **XY**: data frame with spatial coordinates (here: longitude, latitude)
- **P**: data frame with phenotypic traits (here: d13C)
- **G**: data frame with genotypic data (here: 164 SNPs)
- **A**: generated automatically from G: matrix of allele counts (codominant markers only)
- **E**: data frame with environmental data (here: 7 bioclimatic etc. site variables)
- **S**: data frame with structure (hierarchical sampling levels)
- **C**: custom data frame (optional)
- **OUT**: will be used for output generated by methods from package 'EcoGenetics'

### b) Estimate global $F_{ST}$ from SNP data

Let’s examine differentiation for the SNP data using $F_{ST}$. There are a multitude of ways to do this, but we will use the varcomp.glob function from the 'hierfstat' package. 

First, however, we need to re-format the data to the format that 'hierfstat' expects. This is easy with the function 'ecogen2hierfstat'.

```{r}
WWP.hierfstat <- EcoGenetics::ecogen2hierfstat(WWP.ecogen, pop='population', 
                                  to_numeric=TRUE, nout=1)
```

Now we can calculate $F_{ST}$ with the function 'varcomp.glob'.
```{r}
fst <- hierfstat::varcomp.glob(levels = WWP.hierfstat[,1], 
                               loci = WWP.hierfstat[,-1], diploid = T)
head(fst$loc)
fst$overall
fst$F
```

Note: You can also explore bootstrapping across loci to get a confidence interval using the boot.vc() function.

Now, let’s look at the output. The object fst has three elements. 

- 'loc': The first element is matrix of variance components for each SNP. The columns of this matrix are levels you used from the highest to the lowest (left to right). For us, that means column 1 is the variance component for population, column 2 is the variance component for individual, and column 3 is the variance component for the error (or residual). 
- 'overall': The second element is the sum of the columns. 
- 'F': The last element is a matrix of F-statistics. These work by using as subscripts the column title relative to the row title, so the first value on the first line is the F- statistic for population relative to total (i.e. $F_{ST}$). It is calculated based on the variance components from $overall as:

$$F_{pop,tot} = \frac{\sigma^{2}_{pop}}
{\sigma^{2}_{pop}+\sigma^{2}_{ind}+\sigma^{2}_{error}}$$

### c) Compare $Q_{ST}$ to $F_{ST}$

Now that we have inspected overall genetic differentiation among populations, let’s use the 'QstFstComp' library to formally test whether or not $Q_{ST} > F_{ST}$ for $δ^{13}C$.

Note: as this is a permutation test, if you run it several times, the results may change slightly from run to run. 

```{r}
phen_mod <- phen[,-c(2,4)]
QstFst_out <- QstFstComp::QstFstComp(fst.dat = WWP.hierfstat, 
                                     qst.dat = phen_mod, 
                                     numpops = nlevels(WWP@S$population), 
                                     nsim = 10000, 
                                     breeding.design = "half.sib.dam",
                                     dam.offspring.relatedness = 0.25, 
                                     output = "concise_nowrite")

QstFst_out[[1]]
QstFst_out[[3]]
```

The output contains the following elements:

- '[[1]]': The calculated difference between Qst and Fst with 95% critical values.
- '[[2]]': an output file names (though we suppressed the writing of this file with the option 'output="consise_nowrite")'
- '[[3]]': p-values for a hypothesis test with H0: $Q_{ST} = F_{ST}$, with three different alternatives ('less', 'greater', 'two.sided')
- '[[4]]': the Fst value estimated from the data with 95% confidence intervals.
- '[[5]]': the Qst value estimated from the data with 95% confidence intervals.
- '[[6]]': the additive genetic variance for the trait with 95% confidence intervals.

Note: the values are somewhat different from what we calculated, most likely because the function 'QstFstComp' did not account for the block effect.

Inspect the first and third elements of the list 'QstFst_out': 

- For the observed values, is $Q_{ST} > F_{ST}$? 
- Which alternative is most appropriate here?
- Can the null hypothesis be rejected with alpha = 0.05?
- What does this mean biologically? 

## 5. Assess correlation between trait and environment

The last thing we want to do in this part of the lab is to test for correlations between genetic values of $δ^{13}C$ and environmental data.

### a) Correlation matrix

First, we need to combine the trait, geographic and environmental data into a single data frame 'phen_env'. At the same time, we'll use the function 'scale' to scale the geographic and environmental variables, so that each has mean = 0 and a sd = 1. 

```{r}
phen_env <- data.frame(d13c=scale(WWP.ecogen@P[,1]), 
                       scale(WWP.ecogen@XY), scale(WWP.ecogen@E))
```

Fortunately, from the way we imported the data into the 'ecogen' object (with matching row labels), we know that the data in all slots of WWP are ordered by the same variable 'family', so that their rows correspond.

Create a correlation matrix.

```{r}
round(cor(phen_env), 2)
```

- Which site variables show the strongest correlation with the trait?
- Which site variables are strongly correlated with each other? 

### b) Multiple regression model

Next, let’s use multiple regression to test the effect of these variables on $δ^{13}C$. 

```{r}
mod1_env <- lm(d13c ~ longitude + latitude + elev + max_rad + tmax_july + 
                 tmin_jan + ann_ppt + gdd_aug + AWS050, data = phen_env)
summary(mod1_env)
```

This model shows us the effect of all variables on $δ^{13}C$, given all other variables in the model. Note that the estimated slope coefficients 'Estimate', and their p-values 'Pr(>|t|)' may differ from the results of a simple regression analysis (with a single predictor in each model) due to the correlation among predictor variables.

It is important to understand that these estimates and p-values are based on type II sums of squares. This means that for each predictor, they are estimated as if it was added last to the model, thus accounting for the effect of all other variables in the model.

- Is this multiple regression model statistically significant? If so, why? 
- Which variables have a statistically significant effect, given all other variables in the model?
- Which variables provide the largest effects (use the column labeled 'Estimate', which is the partial regression coefficient because we used scaled variables)? Is this consistent with the correlation results?

### c) Variation partitioning

What is the relative contribution of climate vs. geography to variation in the trait $δ^{13}C$? 

This can be assessed with variation partitioning, using the function 'varpart' from package 'vegan'. It takes three (or more) data frames. The first contains the response variable(s), here the trait. The others contain groups of predictors, here the bioclimatic variables and the spatial coordinates, for partitioning the explained variance in the response. We set the argument transfo="standardize" to force all variables to be standardized.

```{r}
par(mar=c(1, 1, 1, 1))
mod <- vegan::varpart(WWP.ecogen@P$d13c, WWP.ecogen@E, WWP.ecogen@XY, 
                      transfo="standardize")
mod
plot(mod)
```

The result is a partition table that lists the size of different fractions of variance. Interpretation should be based on adjusted $R^2$ values. Note that negative $R^2$ values should be interpreted as zero. 

The figure is a graphical representation of the fractions. Here, X1 is the set of bioclimatic variables ('climate'), and X2 is the spatial coordinates ('geography'). 

Fraction [a+b] is the percent of variance in the trait that can be explained by climate. Some of this, however, might also be explained by geography. Specifically, fration [b] is the shared variance between climate and geography, and [b+c] is the percent of variance in the trait that can be explained by geography. Hence, the fraction that can only be explained by climate, but not by geography, is fraction [a]. Similarly, fraction [c] can only be explained by geography but not by climate.

Looks like climate alone ([a] = 47%) explains about three times as much as geography alone ([c] = 15.5%). Together, they explain [a+b+c] = 59% of variation in the trait. Surprisingly, there was no shared variance between climate and geography (b = 0).

Finally, we can test whether each component is statistically different from zero.

First we fit models for all testable fractions (see last column in output above). In a way, we are now making explicit what the function 'varpart' did implicitly. We use the function 'rda' (for 'redundancy analysis') to fit a series of regression models. RDA can take multiple response variables (e.g., allele frequencies of multiple loci, or species abundances in a community), which results in multivariate regression. Here, we have a single response variable (the trait), so each model boils down to a multiple regression with one response and multiple predictor variables. 

The first argument of 'rda' is the response, the second argument is the set of predictors, and the third is an optional set of predictors that should be used for conditioning (i.e., their effect is accounted for before regressing the response on the second set).

```{r}
ab <- vegan::anova.cca(vegan::rda(WWP.ecogen@P$d13c, WWP.ecogen@E,  
                                  transfo="standardize"))
bc <- vegan::anova.cca(vegan::rda(WWP.ecogen@P$d13c, WWP.ecogen@XY, 
                                  transfo="standardize"))
abc <- vegan::anova.cca(vegan::rda(WWP.ecogen@P$d13c, 
                                   data.frame(WWP.ecogen@E, WWP.ecogen@XY),
                                   transfo="standardize"))
a <- vegan::anova.cca(vegan::rda(WWP.ecogen@P$d13c, WWP.ecogen@E, 
                                 WWP.ecogen@XY, transfo="standardize"))
b <- vegan::anova.cca(vegan::rda(WWP.ecogen@P$d13c, WWP.ecogen@XY, 
                                 WWP.ecogen@E, transfo="standardize"))
```

Now we can extract the p-values. Looks like all testable fractions are statistically significant!

```{r}
c(ab=ab$"Pr(>F)"[1], bc=bc$"Pr(>F)"[1], abc=abc$"Pr(>F)"[1], 
  a=a$"Pr(>F)"[1], b=b$"Pr(>F)"[1])
```

```{r message=FALSE, warning=TRUE, include=FALSE}
#detach("package:lme4", unload=TRUE)
#detach("package:Matrix", unload=TRUE)
#detach("package:adegenet", unload=TRUE)
#detach("package:ade4", unload=TRUE)
#detach("package:gstudio", unload=TRUE)
detachAllPackages()
```

